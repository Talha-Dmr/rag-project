# Pilot configuration for SGLD/Langevin on AmbigQA-only NLI data.
# Use with: python scripts/prepare_training_data.py --config sgld_pilot_ambigqa ...

training:
  model:
    base_model: microsoft/deberta-v3-base
    num_labels: 3
    cache_dir: ./models/training

  hyperparameters:
    learning_rate: 5.0e-5
    weight_decay: 0.01
    warmup_steps: 0
    max_epochs: 1
    batch_size: 8
    gradient_accumulation_steps: 1
    max_grad_norm: 1.0
    mixed_precision: fp16
    optimizer: sgld
    noise_scale: 1.0
    scheduler: linear

  data:
    max_seq_length: 128
    train_split: 0.85
    val_split: 0.10
    test_split: 0.05
    balance_classes: true
    balance_strategy: undersample

  checkpointing:
    save_dir: ./models/checkpoints/sgld_pilot_ambigqa
    save_strategy: best
    save_every_n_epochs: 1
    save_total_limit: 2
    metric_for_best: f1_macro
    mode: max

  early_stopping:
    enabled: true
    patience: 1
    metric: f1_macro
    mode: max
    min_delta: 0.0001

  datasets:
    ambigqa:
      path: data/ambiguity_datasets/02_ambigqa
      weight: 1.0
      multiplier: 3

  output:
    training_data_dir: ./data/training/nli_dataset_ambigqa
    checkpoint_dir: ./models/checkpoints/sgld_pilot_ambigqa
    final_model_dir: ./models/hallucination_detector_sgld_pilot_ambigqa
    logs_dir: ./logs/training
