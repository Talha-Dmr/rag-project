# Base Configuration for Modular RAG System

# Data Loading
data_loader:
  type: pdf  # Options: pdf, text, json
  batch_size: 10

  # Loader-specific configs
  pdf:
    extract_images: false

  text:
    encoding: utf-8
    split_by: null  # Options: null, 'paragraph', 'line'

  json:
    content_field: content
    documents_key: null

# Chunking Strategy
chunking:
  strategy: semantic  # Options: semantic, fixed_size
  config:
    # Semantic chunking parameters
    similarity_threshold: 0.7
    min_chunk_size: 100
    max_chunk_size: 1000

    # Embedder for semantic chunking
    embedder:
      type: huggingface
      model_name: sentence-transformers/all-mpnet-base-v2
      device: ${DEVICE:cpu}
      cache_folder: ${EMBEDDING_CACHE_DIR:./models/embeddings}
      batch_size: 32

    # Fixed-size chunking parameters (if using fixed_size strategy)
    # chunk_size: 512
    # chunk_overlap: 50

# Embeddings
embeddings:
  model_name: sentence-transformers/all-mpnet-base-v2
  device: ${DEVICE:cpu}
  cache_folder: ${EMBEDDING_CACHE_DIR:./models/embeddings}
  batch_size: 32

# Vector Store
vector_store:
  type: chroma  # Options: chroma, faiss (more can be added)
  config:
    persist_directory: ${VECTOR_DB_PATH:./data/vector_db/chroma}
    collection_name: rag_documents

# LLM Configuration
llm:
  model_name: ${LLM_MODEL:meta-llama/Llama-2-7b-chat-hf}  # or mistralai/Mistral-7B-Instruct-v0.1
  device: ${DEVICE:cpu}
  cache_folder: ${LLM_CACHE_DIR:./models/llm}
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  load_in_8bit: false  # Set to true for GPU memory optimization

# Retrieval Parameters
retrieval:
  k: 10
  score_threshold: 0.7

# Reranking
reranker:
  type: cross_encoder  # Options: cross_encoder, bm25
  config:
    model_name: cross-encoder/ms-marco-MiniLM-L-6-v2
    top_k: 5
    device: ${DEVICE:cpu}
