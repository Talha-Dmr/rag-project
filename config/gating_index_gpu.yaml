# Index-only config: embedder on GPU, LLM on CPU to avoid VRAM pressure

data_loader:
  type: pdf
  batch_size: 10
  pdf:
    extract_images: false

chunking:
  strategy: fixed_size
  config:
    chunk_size: 512
    chunk_overlap: 50

embeddings:
  type: huggingface
  model_name: sentence-transformers/all-MiniLM-L6-v2
  device: cuda
  cache_folder: ${EMBEDDING_CACHE_DIR:./models/embeddings}
  batch_size: 64

vector_store:
  type: chroma
  config:
    persist_directory: ${VECTOR_DB_PATH:./data/vector_db/gating_demo}
    collection_name: rag_gating_demo

llm:
  model_name: sshleifer/tiny-gpt2
  device: cpu
  cache_folder: ${LLM_CACHE_DIR:./models/llm}
  max_new_tokens: 64
  temperature: 0.7
  top_p: 0.9
  load_in_8bit: false

retrieval:
  k: 5
  score_threshold: 0.0

hallucination_detector:
  model_path: models/checkpoints/sgld_lora_warmstart_ambigqa_mini_noise5e-5/checkpoint-step-5000
  base_model: microsoft/deberta-v3-small
  device: cpu
  max_length: 128
  lora:
    enabled: true
    r: 8
    lora_alpha: 16
    lora_dropout: 0.05
    bias: none
    task_type: SEQ_CLS
    target_modules: ["query_proj", "key_proj", "value_proj"]
    modules_to_save: ["classifier", "pooler"]

gating:
  enabled: true
  strategy: retrieve_more
  contradiction_rate_threshold: 0.5
  contradiction_prob_threshold: 0.7
  uncertainty_threshold: 0.4
  max_retries: 2
  k_multiplier: 2.0
  max_k: 20
  abstain_message: "Bu soruya güvenilir şekilde yanıt veremiyorum."
