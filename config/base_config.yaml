# Base Configuration for Modular RAG System

# Data Loading
data_loader:
  type: pdf  # Options: pdf, text, json
  batch_size: 10

  # Loader-specific configs
  pdf:
    extract_images: false

  text:
    encoding: utf-8
    split_by: null  # Options: null, 'paragraph', 'line'

  json:
    content_field: content
    documents_key: null

# Chunking Strategy
chunking:
  strategy: semantic  # Options: semantic, fixed_size
  config:
    # Semantic chunking parameters
    similarity_threshold: 0.7
    min_chunk_size: 100
    max_chunk_size: 1000

    # Embedder for semantic chunking
    embedder:
      type: huggingface
      model_name: sentence-transformers/all-mpnet-base-v2
      device: ${DEVICE:cpu}
      cache_folder: ${EMBEDDING_CACHE_DIR:./models/embeddings}
      batch_size: 32

    # Fixed-size chunking parameters (if using fixed_size strategy)
    # chunk_size: 512
    # chunk_overlap: 50

# Embeddings
embeddings:
  type: mgte
  model_name: llmrails/mgte-large
  device: ${DEVICE:cuda}

# Vector Store
vector_store:
  type: chroma  # Options: chroma, faiss (more can be added)
  config:
    persist_directory: ${VECTOR_DB_PATH:./data/vector_db/chroma}
    collection_name: rag_documents

# LLM Configuration
llm:
  model_name: ${LLM_MODEL:meta-llama/Llama-2-7b-chat-hf}  # or mistralai/Mistral-7B-Instruct-v0.1
  device: ${DEVICE:cpu}
  cache_folder: ${LLM_CACHE_DIR:./models/llm}
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  load_in_8bit: false  # Set to true for GPU memory optimization

# Retrieval Parameters
retrieval:
  k: 10
  score_threshold: 0.7

# Reranking
reranker:
  type: cross_encoder  # Options: cross_encoder, bm25, ebcar
  config:
    model_name: cross-encoder/ms-marco-MiniLM-L-6-v2
    top_k: 5
    device: ${DEVICE:cpu}
    # For ebcar reranker, use these config fields instead:
    # checkpoint: path/to/ebcar/model.pth
    # embedder_name: mgte  # Options: mgte, sentence_transformers, etc.
    # hidden_size: 768  # Model's hidden size

# Training Configuration for Hallucination Detection Model
training:
  # Model configuration
  model:
    base_model: microsoft/deberta-v3-large-mnli
    num_labels: 3  # entailment, neutral, contradiction
    cache_dir: ./models/training

  # Training hyperparameters
  hyperparameters:
    learning_rate: 2.0e-5
    weight_decay: 0.01
    warmup_steps: 500
    max_epochs: 5
    batch_size: 16
    gradient_accumulation_steps: 4  # Effective batch size = 64
    max_grad_norm: 1.0
    mixed_precision: fp16  # Options: fp16, fp32 (fp16 for 2x speedup)
    optimizer: adamw  # Options: adamw, adam, sgd, sgld
    noise_scale: 1.0  # Langevin noise scale (only for SGLD)
    scheduler: linear  # Options: linear, cosine, constant

  # Data configuration
  data:
    max_seq_length: 256
    train_split: 0.85
    val_split: 0.10
    test_split: 0.05
    balance_classes: true
    balance_strategy: undersample  # Options: undersample, oversample

  # Checkpointing configuration
  checkpointing:
    save_dir: ./models/checkpoints
    save_strategy: best  # Options: epoch, best
    save_every_n_epochs: 1
    save_total_limit: 3
    metric_for_best: val_f1_macro
    mode: max  # Options: max, min

  # Early stopping configuration
  early_stopping:
    enabled: true
    patience: 3
    metric: val_f1_macro
    mode: max
    min_delta: 0.0001

  # Dataset paths and weights
  datasets:
    ambigqa:
      path: data/ambiguity_datasets/02_ambigqa
      weight: 1.0
      multiplier: 3  # 12K → 36K examples

    asqa:
      path: data/ambiguity_datasets/03_asqa
      weight: 1.0
      multiplier: 4  # 5.3K → 21K examples

    wic:
      path: data/ambiguity_datasets/01_wic
      weight: 0.5  # Different task type, lower weight
      multiplier: 2  # 6K → 12K examples

    clamber:
      path: data/ambiguity_datasets/04_clamber
      weight: 1.0
      multiplier: 3  # 3.2K → 9.6K examples

    condambigqa:
      path: data/ambiguity_datasets/05_condambigqa
      weight: 1.0
      multiplier: 3  # 2K → 6K examples

  # Output directories
  output:
    training_data_dir: ./data/training/nli_dataset
    checkpoint_dir: ./models/checkpoints/hallucination_detector
    final_model_dir: ./models/hallucination_detector
    logs_dir: ./logs/training
