{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üöÄ Hallucination Detection Model Training - Colab (OPTIMIZED)\n\n**DeBERTa-v3-BASE ile hƒ±zlƒ± training!**\n\n- **Model:** microsoft/deberta-v3-base (86M params) üöÄ\n- **GPU:** A100 (Colab Pro) veya T4 ‚ö°\n- **Toplam S√ºre:** ~3-4 saat (T4), ~3-4 saat (T4)\n  - Dataset download: ~5 dakika\n  - Dataset conversion: ~2-3 dakika\n  - Training: ~1-1.5 saat (A100) veya ~2-3 saat (T4)\n- **Output:** Google Drive'a otomatik kaydedilir\n\n---\n\n## ‚öôÔ∏è KULLANIM\n1. **Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí A100 (veya T4)**\n2. **Runtime ‚Üí Run all** (Ctrl+F9)\n3. Google Drive izni ver\n4. Bekle ‚òï (~1-2 saat A100 i√ßin)\n5. Model: **Google Drive ‚Üí MyDrive ‚Üí hallucination_model_deberta_base/**\n\n---\n\n## üìä DATASET INFO\n\n### Raw Datasets:\n- **WiC**: 6K examples\n- **AmbigQA**: 12K examples\n- **ASQA**: 5K examples\n- **CLAMBER**: 3K examples\n- **CondAmbigQA**: 2K examples\n\n### After NLI Conversion (NO AUGMENTATION):\n- **Total**: ~28K NLI examples (multiplier=1, orijinal boyut)\n- **Expected batches/epoch**: ~875 (batch_size=16)\n- **Total batches (3 epochs)**: ~2,625\n- **Optimizer steps (grad_accum=2)**: ~1,312\n\n---\n\n## ‚ö° OPTIMIZATIONS\n\n### Model:\n- **DeBERTa-v3-BASE** (86M params - 5x smaller!)\n- **Faster training**: ~1-2 hours (vs 11 hours)\n- **Less memory**: 560MB model (vs 1.6GB)\n\n### Configuration:\n- **Batch size**: 16 (larger for smaller model)\n- **Gradient accumulation**: 4 (effective batch = 64)\n- **Epochs**: 3 (faster iteration)\n- **Sequence length**: 512 (full)\n- **Mixed precision**: FP16\n\n### Performance Optimizations:\n- ‚úì Pre-tokenized cache\n- ‚úì DataLoader num_workers=4\n- ‚úì Prefetch factor=4\n- ‚úì Drop last incomplete batch\n- ‚úì Progress bar shows optimizer steps\n\n### Expected Performance:\n- **Speed**: 2-3 it/s (faster than large model!)\n- **Time**: ~1-2 hours (A100)\n- **Memory**: ~10-15GB\n\n---\n\n## üöÄ READY TO RUN!\n\nOptimize edilmi≈ü bu notebook'u √ßalƒ±≈ütƒ±rmak i√ßin sadece **Run All** yapƒ±n! üéâ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: SETUP & GPU CHECK\n",
    "# ============================================================\n",
    "print(\"=\"*60)\n",
    "print(\"üîß SETUP BA≈ûLIYOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# GPU kontrol\n",
    "import torch\n",
    "print(f\"\\n‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  NO GPU! Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"\\nüìÅ Mounting Google Drive...\")\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Drive mounted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: CLONE CODE & INSTALL DEPENDENCIES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì¶ INSTALLING DEPENDENCIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clone repository\n",
    "print(\"\\n[1/3] Cloning code...\")\n",
    "!cd /content && rm -rf rag-project 2>/dev/null || true\n",
    "!git clone https://github.com/Talha-Dmr/rag-project.git /content/rag-project -q\n",
    "print(\"‚úÖ Code cloned\")\n",
    "\n",
    "# Install dependencies\n",
    "print(\"\\n[2/3] Installing packages...\")\n",
    "!pip install -q pyyaml transformers datasets evaluate scikit-learn accelerate\n",
    "print(\"‚úÖ Packages installed\")\n",
    "\n",
    "# Add to path\n",
    "print(\"\\n[3/3] Setting up path...\")\n",
    "import sys\n",
    "sys.path.insert(0, '/content/rag-project')\n",
    "print(\"‚úÖ Path configured\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 2.5: DOWNLOAD AMBIGUITY DATASETS\n# ============================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"üì• DOWNLOADING 5 AMBIGUITY DATASETS\")\nprint(\"=\"*60)\nprint(\"\\n‚è±Ô∏è  This will take 3-5 minutes...\\n\")\n\nimport os\nfrom pathlib import Path\nimport json\n\n# Create base directory\nbase_dir = Path('/content/rag-project/data/ambiguity_datasets')\nbase_dir.mkdir(parents=True, exist_ok=True)\n\n# ============== 1. WiC Dataset ==============\nprint(\"[1/5] Downloading WiC dataset...\")\nwic_dir = base_dir / '01_wic'\nwic_dir.mkdir(exist_ok=True)\n\n!wget -q https://pilehvar.github.io/wic/package/WiC_dataset.zip -O /tmp/wic.zip\n!unzip -q /tmp/wic.zip -d /tmp/wic_extract\n!cp -r /tmp/wic_extract/train {str(wic_dir)}/\n!cp -r /tmp/wic_extract/dev {str(wic_dir)}/\n!rm -rf /tmp/wic.zip /tmp/wic_extract\n\nprint(\"‚úÖ WiC: ~6K examples\")\n\n# ============== 2. AmbigQA Dataset ==============\nprint(\"\\n[2/5] Downloading AmbigQA dataset...\")\nambigqa_dir = base_dir / '02_ambigqa'\nambigqa_dir.mkdir(exist_ok=True)\n\n!wget -q https://nlp.cs.washington.edu/ambigqa/data/train_light.json -O {str(ambigqa_dir)}/train_light.json\n!wget -q https://nlp.cs.washington.edu/ambigqa/data/dev_light.json -O {str(ambigqa_dir)}/dev_light.json\n\nprint(\"‚úÖ AmbigQA: ~12K examples\")\n\n# ============== 3. ASQA Dataset ==============\nprint(\"\\n[3/5] Downloading ASQA dataset...\")\nfrom datasets import load_dataset\n\nasqa_dir = base_dir / '03_asqa' / 'dataset'\nasqa_dir.mkdir(parents=True, exist_ok=True)\n\nasqa_ds = load_dataset('din0s/asqa', split='train')\n# Save as JSON with train/dev splits\ntrain_data = [ex for ex in asqa_ds.select(range(int(len(asqa_ds)*0.82)))]\ndev_data = [ex for ex in asqa_ds.select(range(int(len(asqa_ds)*0.82), len(asqa_ds)))]\n\nasqa_combined = {'train': train_data, 'dev': dev_data}\nwith open(asqa_dir / 'ASQA.json', 'w') as f:\n    json.dump(asqa_combined, f)\n\nprint(\"‚úÖ ASQA: ~5K examples\")\n\n# ============== 4. CLAMBER Dataset ==============\nprint(\"\\n[4/5] Downloading CLAMBER dataset...\")\nclamber_dir = base_dir / '04_clamber'\nclamber_dir.mkdir(exist_ok=True)\n\n# Download from GitHub raw URL\n!wget -q https://raw.githubusercontent.com/yuchenlin/clamber/main/clamber_benchmark.jsonl -O {str(clamber_dir)}/clamber_benchmark.jsonl\n\nprint(\"‚úÖ CLAMBER: ~3K examples\")\n\n# ============== 5. CondAmbigQA Dataset ==============\nprint(\"\\n[5/5] Downloading CondAmbigQA dataset...\")\ncondambigqa_dir = base_dir / '05_condambigqa'\ncondambigqa_dir.mkdir(exist_ok=True)\n\ncondambigqa_ds = load_dataset('Apocalypse-AGI-DAO/CondAmbigQA-2K', split='train')\n# Save as JSON\ncondambigqa_data = [ex for ex in condambigqa_ds]\nwith open(condambigqa_dir / 'train.json', 'w') as f:\n    json.dump(condambigqa_data, f)\n\nprint(\"‚úÖ CondAmbigQA: ~2K examples\")\n\n# Summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ ALL DATASETS DOWNLOADED!\")\nprint(\"=\"*60)\nprint(\"\\nüìä Summary:\")\nprint(\"   ‚Ä¢ WiC: ~6K examples\")\nprint(\"   ‚Ä¢ AmbigQA: ~12K examples\")\nprint(\"   ‚Ä¢ ASQA: ~5K examples\")\nprint(\"   ‚Ä¢ CLAMBER: ~3K examples\")\nprint(\"   ‚Ä¢ CondAmbigQA: ~2K examples\")\nprint(\"   ‚Ä¢ TOTAL: ~28K raw examples ‚Üí ~95K NLI examples (after conversion)\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 4: PREPARE REAL NLI DATASET FROM 5 DATASETS\n# ============================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"üìä PREPARING REAL NLI DATASET FROM 5 AMBIGUITY DATASETS\")\nprint(\"=\"*60)\nprint(\"\\n‚è±Ô∏è  This will take 5-10 minutes...\\n\")\n\nimport os\nimport json\nfrom pathlib import Path\nimport sys\nsys.path.insert(0, '/content/rag-project')\n\n# Import converters\nfrom src.training.data.converters.ambigqa_converter import AmbigQAConverter\nfrom src.training.data.converters.asqa_converter import ASQAConverter\nfrom src.training.data.converters.wic_converter import WiCConverter\nfrom src.training.data.converters.clamber_converter import CLAMBERConverter\nfrom src.training.data.converters.condambigqa_converter import CondAmbigQAConverter\n\n# Create output directory\nos.makedirs('/content/nli_dataset', exist_ok=True)\n\n# Dataset paths (after git clone)\nbase_path = Path('/content/rag-project/data/ambiguity_datasets')\n\n# Initialize converters with actual dataset paths\nconverters = [\n    ('AmbigQA', AmbigQAConverter(str(base_path / '02_ambigqa'), multiplier=1)),\n    ('ASQA', ASQAConverter(str(base_path / '03_asqa'), multiplier=1)),\n    ('WiC', WiCConverter(str(base_path / '01_wic'), multiplier=1)),\n    ('CLAMBER', CLAMBERConverter(str(base_path / '04_clamber'), multiplier=1)),\n    ('CondAmbigQA', CondAmbigQAConverter(str(base_path / '05_condambigqa'), multiplier=1))\n]\n\nall_examples = []\ndataset_stats = {}\n\n# Convert each dataset\nfor name, converter in converters:\n    print(f\"\\nüîÑ Converting {name}...\")\n    try:\n        examples = converter.convert()\n        all_examples.extend(examples)\n        dataset_stats[name] = len(examples)\n        print(f\"   ‚úÖ {len(examples):,} examples from {name}\")\n    except Exception as e:\n        print(f\"   ‚ö†Ô∏è  Error with {name}: {e}\")\n        import traceback\n        traceback.print_exc()\n\n# ========== LIMIT DATASET SIZE ==========\n# Converter'lar √ßok √∂rnek √ºretiyor, 30K ile sƒ±nƒ±rla\nimport random\nrandom.seed(42)\n\nMAX_EXAMPLES = 30000\nif len(all_examples) > MAX_EXAMPLES:\n    print(f\"\\n‚ö†Ô∏è  Dataset too large ({len(all_examples):,}), sampling {MAX_EXAMPLES:,} examples...\")\n    all_examples = random.sample(all_examples, MAX_EXAMPLES)\n    print(f\"‚úÖ Sampled to {len(all_examples):,} examples\")\n\n# Save to JSONL (train file)\noutput_file = '/content/nli_dataset/train.jsonl'\nprint(f\"\\nüíæ Saving to {output_file}...\")\nwith open(output_file, 'w', encoding='utf-8') as f:\n    for ex in all_examples:\n        # Remove metadata to reduce file size\n        clean_ex = {\n            'premise': ex['premise'],\n            'hypothesis': ex['hypothesis'],\n            'label': ex['label']\n        }\n        f.write(json.dumps(clean_ex, ensure_ascii=False) + '\\n')\n\n# Create minimal val file (we'll use val_split=0 in trainer)\nval_file = '/content/nli_dataset/val.jsonl'\nwith open(val_file, 'w', encoding='utf-8') as f:\n    # Use first 100 examples for validation\n    for ex in all_examples[:100]:\n        clean_ex = {\n            'premise': ex['premise'],\n            'hypothesis': ex['hypothesis'],\n            'label': ex['label']\n        }\n        f.write(json.dumps(clean_ex, ensure_ascii=False) + '\\n')\n\n# Print statistics\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ DATASET READY!\")\nprint(\"=\"*60)\nprint(f\"\\nüìä Statistics:\")\nprint(f\"   ‚Ä¢ Total examples: {len(all_examples):,}\")\nfor dataset_name, count in dataset_stats.items():\n    percentage = (count / len(all_examples)) * 100\n    print(f\"   ‚Ä¢ {dataset_name}: {count:,} ({percentage:.1f}%)\")\n\n# Label distribution\nfrom collections import Counter\nlabel_counts = Counter(ex['label'] for ex in all_examples)\nlabel_names = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\nprint(f\"\\nüìà Label Distribution:\")\nfor label, count in sorted(label_counts.items()):\n    percentage = (count / len(all_examples)) * 100\n    print(f\"   ‚Ä¢ {label_names[label]}: {count:,} ({percentage:.1f}%)\")\n\nprint(f\"\\nüìÅ Files:\")\nprint(f\"   ‚Ä¢ Train: {output_file}\")\nprint(f\"   ‚Ä¢ Val: {val_file}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 5: CONFIG (DeBERTa-BASE OPTIMIZED)\n# ============================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚öôÔ∏è  CONFIGURATION - DeBERTa-BASE OPTIMIZED\")\nprint(\"=\"*60)\n\nimport yaml\nimport json\nimport os\n\n# Create DeBERTa-BASE config\nconfig = {\n    'model': {\n        'base_model': 'microsoft/deberta-v3-base',\n        'num_labels': 3\n    },\n    'hyperparameters': {\n        'num_epochs': 3,\n        'batch_size': 16,             # Larger batch for smaller model\n        'learning_rate': 2e-5,\n        'gradient_accumulation_steps': 4,  # 32 √ó 2 = 64 effective\n        'warmup_ratio': 0.1,\n        'weight_decay': 0.01,\n        'max_grad_norm': 1.0,\n        'mixed_precision': 'fp16'\n    },\n    'data': {\n        'max_seq_length': 512,        # Full sequence length\n        'dataset_names': ['ambigqa', 'asqa', 'wic', 'clamber', 'condambigqa'],\n        'balance_classes': True\n    },\n    'checkpoint': {\n        'save_dir': '/content/checkpoints',\n        'save_every_n_batches': 200,\n        'save_total_limit': 2\n    }\n}\n\n# Save config\nos.makedirs('/content/rag-project/config', exist_ok=True)\nwith open('/content/rag-project/config/hallucination_training.yaml', 'w') as f:\n    yaml.dump(config, f)\n\nprint(\"‚úÖ Config created (DeBERTa-BASE OPTIMIZED)\")\nprint(f\"   ‚Ä¢ Model: {config['model']['base_model']}\")\nprint(f\"   ‚Ä¢ Batch size: {config['hyperparameters']['batch_size']}\")\nprint(f\"   ‚Ä¢ Gradient accumulation: {config['hyperparameters']['gradient_accumulation_steps']}\")\nprint(f\"   ‚Ä¢ Effective batch: {config['hyperparameters']['batch_size'] * config['hyperparameters']['gradient_accumulation_steps']}\")\nprint(f\"   ‚Ä¢ Epochs: {config['hyperparameters']['num_epochs']}\")\nprint(f\"   ‚Ä¢ Max sequence length: {config['data']['max_seq_length']}\")\nprint(f\"   ‚Ä¢ Mixed precision: {config['hyperparameters']['mixed_precision']}\")\nprint(f\"\\n‚ö° EXPECTED PERFORMANCE:\")\nprint(f\"   ‚Ä¢ Speed: 2-3 it/s\")\nprint(f\"   ‚Ä¢ Time: ~3-4 hours (T4)\")\nprint(f\"   ‚Ä¢ Memory: ~10-15GB\")\nprint(f\"   ‚Ä¢ Model size: 560MB (vs 1.6GB)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 6: CHECK & RE-CREATE CACHE IF NEEDED\n# ============================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"üîç CHECKING & CREATING CACHE\")\nprint(\"=\"*60)\n\nimport os\nfrom pathlib import Path\n\n# Check cache files\ncache_dir = Path('/content/nli_dataset_cache')\ntrain_cache = cache_dir / 'train.pt'\nval_cache = cache_dir / 'val.pt'\n\nif train_cache.exists() and val_cache.exists():\n    print(\"‚úÖ Cache files exist!\")\n    print(f\"   ‚Ä¢ Train cache: {train_cache.stat().st_size / 1024**3:.1f}GB\")\n    print(f\"   ‚Ä¢ Val cache: {val_cache.stat().st_size / 1024**2:.1f}MB\")\n    \nelse:\n    print(\"‚ùå Cache files missing! Creating now...\")\n    \n    # Load raw data and create cache\n    import torch\n    import json\n    from transformers import AutoTokenizer\n    from tqdm import tqdm\n    \n    print(\"üìÇ Loading raw data...\")\n    with open('/content/nli_dataset/train.jsonl', 'r') as f:\n        train_data = [json.loads(line) for line in f]\n    \n    with open('/content/nli_dataset/val.jsonl', 'r') as f:\n        val_data = [json.loads(line) for line in f]\n    \n    print(f\"   ‚Ä¢ Train examples: {len(train_data):,}\")\n    print(f\"   ‚Ä¢ Val examples: {len(val_data):,}\")\n    \n    # Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n    \n    def tokenize_batch(examples, max_length=512):\n        premises = [ex['premise'] for ex in examples]\n        hypotheses = [ex['hypothesis'] for ex in examples]\n        labels = [ex['label'] for ex in examples]\n        \n        encodings = tokenizer(\n            premises, hypotheses,\n            max_length=max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encodings['input_ids'],\n            'attention_mask': encodings['attention_mask'],\n            'labels': torch.tensor(labels, dtype=torch.long)\n        }\n    \n    # Tokenize train data\n    print(\"\\nüöÄ Tokenizing train data (this will take 3-5 minutes)...\")\n    batch_size = 1000\n    train_tokenized = []\n    \n    for i in tqdm(range(0, len(train_data), batch_size), desc=\"Tokenizing train\"):\n        batch = train_data[i:i+batch_size]\n        tokenized = tokenize_batch(batch)\n        train_tokenized.append(tokenized)\n    \n    # Combine batches\n    print(\"üîó Combining batches...\")\n    train_input_ids = torch.cat([t['input_ids'] for t in train_tokenized], dim=0)\n    train_attention_mask = torch.cat([t['attention_mask'] for t in train_tokenized], dim=0)\n    train_labels = torch.cat([t['labels'] for t in train_tokenized], dim=0)\n    \n    # Tokenize validation data\n    print(\"\\nüîß Tokenizing validation data...\")\n    val_tokenized = tokenize_batch(val_data)\n    \n    # Save cache\n    print(\"\\nüíæ Saving to cache...\")\n    cache_dir.mkdir(exist_ok=True)\n    \n    torch.save({\n        'input_ids': train_input_ids,\n        'attention_mask': train_attention_mask,\n        'labels': train_labels\n    }, train_cache)\n    \n    torch.save(val_tokenized, val_cache)\n    \n    print(f\"\\n‚úÖ Cache created!\")\n    print(f\"   ‚Ä¢ Train cache: {train_input_ids.shape}\")\n    print(f\"   ‚Ä¢ Val cache: {val_tokenized['input_ids'].shape}\")\n    print(f\"   ‚Ä¢ Cache size: {train_cache.stat().st_size / 1024**3:.1f}GB\")\n\nprint(\"‚úÖ Cache ready for training!\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 7: FINAL TRAINING - A100 OPTIMIZED WITH CACHE\n# ============================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"üî• FINAL TRAINING - DeBERTa-BASE OPTIMIZED\")\nprint(\"=\"*60)\n\nimport yaml\nfrom src.training.trainers.hallucination_trainer import HallucinationTrainer\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# Cache'den okuyan optimize DataLoader\nclass CachedNLIDataset(Dataset):\n    \"\"\"Dataset that loads pre-tokenized data from cache.\"\"\"\n    \n    def __init__(self, cache_file):\n        self.data = torch.load(cache_file)\n        self.length = self.data['input_ids'].shape[0]\n    \n    def __len__(self):\n        return self.length\n    \n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.data['input_ids'][idx],\n            'attention_mask': self.data['attention_mask'][idx],\n            'labels': self.data['labels'][idx]\n        }\n\ndef cached_collate_fn(batch):\n    \"\"\"Fast collate function for cached data.\"\"\"\n    return {\n        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n        'labels': torch.stack([item['labels'] for item in batch])\n    }\n\ndef create_cached_dataloader(cache_file, batch_size=16, shuffle=True):\n    \"\"\"Create OPTIMIZED DataLoader from cache (A100 Colab tuned).\"\"\"\n    dataset = CachedNLIDataset(cache_file)\n    \n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        num_workers=4,              # ‚ö° 8 ‚Üí 4 (optimal for A100 Colab)\n        pin_memory=True,\n        prefetch_factor=4,          # ‚ö° 2 ‚Üí 4 (aggressive prefetching)\n        persistent_workers=True,\n        drop_last=True,             # ‚ö° NEW: Drop incomplete last batch\n        collate_fn=cached_collate_fn\n    )\n\n# Load config directly\nwith open('/content/rag-project/config/hallucination_training.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\n# üîç BATCH SIZE VERIFICATION\nprint(f\"\\nüîç CONFIGURATION VERIFICATION:\")\nprint(f\"   ‚Ä¢ Batch size: {config['hyperparameters']['batch_size']}\")\nprint(f\"   ‚Ä¢ Gradient accumulation: {config['hyperparameters']['gradient_accumulation_steps']}\")\nprint(f\"   ‚Ä¢ Effective batch size: {config['hyperparameters']['batch_size'] * config['hyperparameters']['gradient_accumulation_steps']}\")\nprint(f\"   ‚Ä¢ Mixed precision: {config['hyperparameters']['mixed_precision']}\")\nprint(f\"   ‚Ä¢ Max seq length: {config['data']['max_seq_length']}\")\n\n# Initialize trainer\ntrainer = HallucinationTrainer(config)\nprint(\"\\n‚úÖ Trainer initialized\")\n\n# Build model\ntrainer.build_model()\nprint(\"‚úÖ Model built\")\n\n# Patch trainer to use cached data\nprint(\"\\nüîß Patching trainer to use OPTIMIZED cached dataloaders...\")\noriginal_prepare_data = trainer.prepare_data\n\ndef cached_prepare_data(train_path, val_path):\n    \"\"\"Override prepare_data to use OPTIMIZED cached data.\"\"\"\n    print(\"üöÄ Using pre-tokenized cached data with A100 optimizations!\")\n    \n    trainer.train_loader = create_cached_dataloader(\n        '/content/nli_dataset_cache/train.pt',\n        batch_size=16,\n        shuffle=True\n    )\n    trainer.val_loader = create_cached_dataloader(\n        '/content/nli_dataset_cache/val.pt',\n        batch_size=16,\n        shuffle=False\n    )\n    \n    # üìä DATALOADER INFO\n    print(f\"\\nüìä DATALOADER INFO:\")\n    print(f\"   ‚Ä¢ Train batches/epoch: {len(trainer.train_loader):,}\")\n    print(f\"   ‚Ä¢ Val batches: {len(trainer.val_loader):,}\")\n    print(f\"   ‚Ä¢ Total batches (3 epochs): {len(trainer.train_loader) * 3:,}\")\n    print(f\"   ‚Ä¢ Optimizer steps (grad_accum=4): {len(trainer.train_loader) * 5 // 4:,}\")\n    print(f\"\\n‚ö° OPTIMIZATIONS:\")\n    print(f\"   ‚Ä¢ num_workers: 4 (Colab A100 optimal)\")\n    print(f\"   ‚Ä¢ prefetch_factor: 4 (aggressive)\")\n    print(f\"   ‚Ä¢ drop_last: True (skip incomplete batch)\")\n\n# Apply the patch\ntrainer.prepare_data = cached_prepare_data\n\n# Prepare data (will use cached data!)\ntrainer.prepare_data('/content/nli_dataset/train.jsonl', '/content/nli_dataset/val.jsonl')\nprint(\"\\n‚úÖ Cached data prepared\")\n\n# START TRAINING\nprint(\"\\n\" + \"=\"*60)\nprint(\"üöÄ TRAINING STARTING...\")\nprint(\"=\"*60)\nprint(f\"\\n‚ö° Expected Performance:\")\nprint(f\"   ‚Ä¢ Speed: 2-3 it/s (faster with smaller model!)\")\nprint(f\"   ‚Ä¢ Time: ~3-4 hours (T4)\")\nprint(f\"   ‚Ä¢ Memory: ~8-10GB\")\nprint(f\"\\nüìä Progress bar shows OPTIMIZER STEPS (not batches!)\")\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\nhistory = trainer.train(\n    num_epochs=3,\n    output_dir='/content/output'\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 8: SAVE TO GOOGLE DRIVE\n# ============================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"üíæ SAVING MODEL TO GOOGLE DRIVE\")\nprint(\"=\"*60)\n\nimport os\nimport json\n\n# Model'i kaydet\nsave_path = '/content/drive/MyDrive/hallucination_model_deberta_base'\nos.makedirs(save_path, exist_ok=True)\n\nprint(\"\\n[1/3] Saving model...\")\ntrainer.model.save_pretrained(save_path)\nprint(\"‚úÖ Model saved\")\n\nprint(\"\\n[2/3] Saving tokenizer...\")\ntrainer.tokenizer.save_pretrained(save_path)\nprint(\"‚úÖ Tokenizer saved\")\n\nprint(\"\\n[3/3] Saving metrics...\")\nwith open(f'{save_path}/final_metrics.json', 'w') as f:\n    json.dump({\n        'validation_metrics': history.get('val_metrics', [])[-1] if history.get('val_metrics') else {},\n        'training_config': {\n            'num_epochs': 3,\n            'batch_size': 32,\n            'gradient_accumulation_steps': 2,\n            'effective_batch_size': 64,\n            'learning_rate': 2e-5,\n            'max_seq_length': 512,\n            'model': 'microsoft/deberta-v3-base',\n            'optimization': 'DeBERTa-BASE - 86M params, batch=32, 3 epochs, ~28K examples'\n        }\n    }, f, indent=2)\nprint(\"‚úÖ Metrics saved\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üéâ TAMAMLANDI!\")\nprint(\"=\"*60)\nprint(f\"\\nüìÅ Model konumu:\")\nprint(f\"   Google Drive ‚Üí MyDrive ‚Üí hallucination_model_deberta_base/\")\nprint(f\"\\nüìä Final Metrics:\")\nif history.get('val_metrics'):\n    for key, value in history['val_metrics'][-1].items():\n        if 'accuracy' in key or 'f1' in key:\n            print(f\"   ‚Ä¢ {key}: {value:.4f}\")\nprint(f\"\\n‚ö° DeBERTa-BASE TRAINING SUMMARY:\")\nprint(\"   ‚Ä¢ Model: microsoft/deberta-v3-base (86M params)\")\nprint(\"   ‚Ä¢ Dataset: ~28K examples (multiplier=1)\")\nprint(\"   ‚Ä¢ Epochs: 3\")\nprint(\"   ‚Ä¢ Batch size: 32 (effective: 64)\")\nprint(\"\\n‚úÖ Model Google Drive'da g√ºvende!\")\nprint(\"=\"*60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}