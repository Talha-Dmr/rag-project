# Macro outlooks domain (World Bank + UN WESP)

data_loader:
  type: pdf
  batch_size: 10
  pdf:
    extract_images: false

chunking:
  strategy: fixed_size
  config:
    chunk_size: 512
    chunk_overlap: 50

embeddings:
  type: huggingface
  model_name: sentence-transformers/all-MiniLM-L6-v2
  device: ${DEVICE:cpu}
  cache_folder: ${EMBEDDING_CACHE_DIR:./models/embeddings}
  batch_size: 32

vector_store:
  type: chroma
  config:
    persist_directory: ${VECTOR_DB_PATH:./data/vector_db/macro_outlooks}
    collection_name: rag_macro_outlooks

llm:
  model_name: Qwen/Qwen2.5-1.5B-Instruct
  device: ${DEVICE:cuda}
  cache_folder: ${LLM_CACHE_DIR:./models/llm}
  max_new_tokens: 128
  temperature: 0.7
  top_p: 0.9
  load_in_8bit: false

retrieval:
  k: 5
  score_threshold: 0.0

reranker:
  type: ebcar
  top_k: 5
  weights:
    semantic: 0.6
    retriever: 0.2
    position: 0.1
    evidence: 0.07
    length: 0.03

hallucination_detector:
  model_path: models/checkpoints/sgld_lora_swag_collect_ambigqa_mini/checkpoint-epoch-2
  base_model: microsoft/deberta-v3-small
  device: cpu
  max_length: 128
  swag:
    enabled: true
    path: models/checkpoints/sgld_lora_swag_collect_ambigqa_mini/swag_stats.pt
    num_samples: 5
  lora:
    enabled: true
    r: 8
    lora_alpha: 16
    lora_dropout: 0.05
    bias: none
    task_type: SEQ_CLS
    target_modules: ["query_proj", "key_proj", "value_proj"]
    modules_to_save: ["classifier", "pooler"]

gating:
  enabled: true
  strategy: retrieve_more   # options: retrieve_more, abstain
  uncertainty_source: entropy
  contradiction_rate_threshold: 0.50
  contradiction_prob_threshold: 0.70
  uncertainty_threshold: 0.42
  min_retrieval_score: 0.15
  min_mean_retrieval_score: 0.08
  max_retries: 2
  k_multiplier: 2.0
  max_k: 20
  abstain_message: "Bu soruya güvenilir şekilde yanıt veremiyorum."
